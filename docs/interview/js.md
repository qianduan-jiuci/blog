# js面试题
## 小数运算不精确的根源
这并不算是js的缺陷，其实是早期语言(遵循IEEE754规范的语言)都有的问题，因为在处理小数的时候，计算机会遇到一些难题，我们会想当然的认为，`小数运算就小数运算嘛，不精确就不精确嘛，为什么有的精确有的不精确？`

计算机在进行数字运算的时候，是需要转成二进制运算的，一转二进制就要出问题。 


我们回想当然的认为， 小数转二进制和整数转二进制是一样的，比如说
````js
3 -> 11
3.3 -> 11.11
// 其实不是这样的，如果是这样的话，是要出问题的
// 当3.3 + 3.3 = 6.6， 但二进制的计算过程中：11.11+11.11 = 1111.10 转成十进制变成了7.1
````

如果按照这样的运算规则的话，为了保证二进制的精确运算，那么计算机就不得不针对小数运算搞一套独有的规则，这些CPU的厂家肯定不会这样干的，太消耗cpu的性能了
，所以他期望的运算规则是整数和小数是一样的， 那么计算机就要对于二进制的转换规则做文章了，选择了下面的计算规则
````js
十进制
314 = 3 * 102 + 1 * 101 + 4 * 100 （2，1，0是n次方） 
3.14 = 3 * 100 + 1 * 10-1 + 4 * 10-2

二进制
101 = 1 * 102 + 0 * 101 + 1*100
1.01 = 1 * 100 + 0 * 10-1 + 1 * 10-2 = 1.625
````

只有按照这样的转换规则(和十进制的展开逻辑是一样的)，只有使用这样的展开逻辑，他们的运算规则加减乘除才能一直保持一致，这样的话其实有个特点可以发现
`只有十进制为5结尾的数字，才能转换成有限位数的二进制数,如果十进制数字不是以5结尾的数字，那即便转成二进制也是无限位数的二进制数，计算机不会允许这样的情况出现的，也没办法存储，所以，计算机会对这些无限位数的小数进行截取，如果以0截取，那么会比原来的值要小，反之更大，这也是为什么小数不精确的根源所在`







## js计时器是否精确？ 为什么？
肯定是不精确的
至于为什么要从下面四个方面解释
- 硬件
- 系统
- 标准
- 事件循环

### 硬件？
世界上根本没有绝对精确的时间，但是有相对精确的计时工具（原子钟，通过原子的振动频率来进行计时的），计算机可能使用原子钟吗，不可能的，计算机使用寄存器来计时工具
### 系统
不管是什么系统，浏览器都不会去参与数字运算的，浏览器通过调用操作系统的接口来获取运算结果 ，不同的操作系统有不同的计时时限，不管是哪个计时实现，它都是不精确的
### 标准
w3c标准表明计时嵌套层级如果大于了五层，那么它的计时间隔最小为4ms
### 事件循环
不管是settimeout还是setinterval，都是在任务队列中等待执行的，时间到了只能说明是进入到了任务队列中了，还需要等待，这个时间也是不精确的

